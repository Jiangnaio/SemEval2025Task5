{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Code': 'gnd:4003694-7', 'Classification Number': '00', 'Classification Name': 'Unspezifische Allgemeinwörter', 'Name': 'Ausbreitung', 'Alternate Name': [], 'Related Subjects': [], 'Source': 'Du.'}\n",
      "gnd:1163295434 Airfix Marke\n",
      "len(train_datas)= 81937\n",
      "len(dev_datas)= 13666\n",
      "len(datas)= 81937\n",
      "{'title': 'Empowerment in der sozialen Arbeit : eine Einführung', 'abstract': ['Das Empowerment-Konzept formuliert eine deutliche Abkehr vom Defizit-Blickwinkel auf die Adressaten Sozialer Arbeit. Handlungsziel der Empowerment-Praxis ist es, die vorhandenen Fähigkeiten der Adressaten sozialer Dienstleistungen zu autonomer Alltagsregie und Lebensorganisation zu kräftigen und Ressourcen freizusetzen, mit deren Hilfe sie die eigenen Lebenswege selbstbestimmt gestalten können. Das Buch bietet eine leicht verständliche Einführung in Theorie und Praxis des Empowerment in der Sozialen Arbeit. Die vielfältigen Methoden, die in der Empowermentpraxis zum Einsatz kommen, werden ausführlich dargestellt: Unterstützungsmanagement, Biographischer Dialog, Netzwerkarbeit, Organisationsentwicklung. Positionsbestimmungen zur aktuellen Debatte über die veränderte professionelle Identität der Sozialen Arbeit im Zeichen des Empowerment runden das Buch ab.', 'The concept of empowerment represents a change from a deficit centred perspective on recipients of social work. The practice of empowerment aims at strengthening the skills of recipients to manage their own everyday lives and liberating resources to organise their own way of life. The book offers a comprehensive introduction to the theory and practice of empowerment in social work. The multiple methods used in the empowerment practice are presented in detail: management of support, biographic dialogue, networking and organisational development. Finally the book undertakes an effort to position itself in the current debate on the changes of the identity of social work through empowerment.  Prof. Herringer teaches sociology of social work at the Fachhochschule Düsseldorf.'], 'dcterms:subject': [{'@id': 'gnd:4055676-1'}, {'@id': 'gnd:4474644-1'}], 'subject': ['(classificationName=rvk)DS 1100', '(classificationName=bk, id=106421948)79.03 - Methoden und Techniken der Sozialpädagogik und Sozialarbeit', '(classificationName=ddc)361.3', '(classificationName=bk, id=106412892)79.30 - Beratungskunde', '(classificationName=rvk)DS 5600', 'Electronic books', '(classificationName=linseach:mapping)pae', '(classificationName=rvk)DS 5000', '(classificationName=ddc-dbn)360', '(classificationName=loc)HV10.5', '(classificationName=bk, id=106406728)79.02 - Theorie der Sozialpädagogik und Sozialarbeit']}\n",
      "{'title': 'Finanzierung', 'abstract': 'Das Lehrbuch aus der Reihe \"Kompendium der praktischen Betriebswirtschaft\" informiert ausführlich über die Möglichkeiten der Kapitalbeschaffung und -kontrolle. Mit seinem Übungsteil richtet es sich an Studierende der Wirtschaftswissenschaften, die sich gezielt auf ihre Prüfungen vorbereiten wollen. Rezension (ekz): Alle Bände der Reihe sind gleichartig aufgebaut. Einem Theorieteil folgen Übungsaufgaben und die entsprechenden Lösungen. Der vorliegende Band behandelt Themen wie Finanzplanung, Zahlungsverkehr, Beteiligungsfinanzierung, Fremdfinanzierung, Innenfinanzierung und finanzwirtschaftliche Analyse. Da der Kapitalbeschaffung und Kapitalkontrolle in Unternehmen, aufgrund der verkürzten Produktlebenszyklen, der schnell fortschreitenden technischen Entwicklung und des harten Wettbewerbs, grosse Bedeutung zukommt, ist die Finanzierung ein besonders wichtiger Themenbereich für Studierende der Wirtschaftswissenschaften. Die Vorauflage kann auslaufen. (2)', 'dcterms:subject': {'@id': 'gnd:4017182-6'}, 'subject': ['Finanzierung', 'Betriebliche Finanzwirtschaft', '(classificationName=ddc-dbn)650', '(classificationName=bk, id=106414763)85.30 - Investition, Finanzierung', '(classificationName=linseach:mapping)oek', 'Lehrbuch', '(classificationName=ddc)658.15', '(classificationName=loksys-fbw)oek 8125', '(classificationName=rvk)QP 700', 'Unternehmensfinanzierung']}\n",
      "211393\n",
      "211393\n",
      "81937\n",
      "204739\n",
      "31664\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "root='/media/jh/新加卷/2024_11_10/llms4subjects-main/shared-task-datasets/TIBKAT/'\n",
    "all_core='all-subjects'\n",
    "def process_data(root,dev,en_de,datas):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'subject' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        item['title']=tmp['title']\n",
    "        item['abstract']=tmp['abstract']\n",
    "        item['dcterms:subject']=tmp['dcterms:subject']\n",
    "        item['subject']=tmp['subject']\n",
    "        datas.append(item)\n",
    "    for dirs in os.listdir(root+f'{all_core}/data/{dev}'):\n",
    "        for file in os.listdir(root+f'{all_core}/data/{dev}/{dirs}/{en_de}'):\n",
    "            get_train_data(root+f'{all_core}/data/{dev}/{dirs}/{en_de}/{file}')\n",
    "     \n",
    "train_datas = []\n",
    "en_de='en'\n",
    "dev='train'\n",
    "process_data(root,dev,en_de,train_datas)\n",
    "en_de='de'\n",
    "dev='train'\n",
    "process_data(root,dev,en_de,train_datas)\n",
    "random.shuffle(train_datas)\n",
    "\n",
    "dev_datas=[]\n",
    "en_de='en'\n",
    "dev='dev'\n",
    "process_data(root,dev,en_de,dev_datas)\n",
    "en_de='de'\n",
    "dev='dev'\n",
    "process_data(root,dev,en_de,dev_datas)\n",
    "random.shuffle(dev_datas)\n",
    "\n",
    "import json\n",
    "root='/media/jh/新加卷'\n",
    "file_path=f'{root}/2024_11_10/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json' #GND-Subjects-tib-core.json'\n",
    "with open(file_path,'r') as f:\n",
    "    data=json.loads(f.read())\n",
    "\n",
    "print(data[0])\n",
    "gnd_id2name={}\n",
    "for i in data:\n",
    "    gnd_id2name[i['Code']]=i['Name']\n",
    "    for j in i['Alternate Name']:\n",
    "        gnd_id2name[i['Code']]=gnd_id2name[i['Code']]+','+j\n",
    "print(i['Code'],gnd_id2name[i['Code']])\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "print('len(train_datas)=',len(train_datas))\n",
    "print('len(dev_datas)=',len(dev_datas))\n",
    "datas=train_datas\n",
    "print('len(datas)=',len(datas))\n",
    "# datas数据样本随机打乱\n",
    "import random\n",
    "print(datas[0])\n",
    "random.shuffle(datas)\n",
    "print(datas[0])\n",
    "x_train=[]\n",
    "s1s=[]\n",
    "cnt=0\n",
    "gnd_set=set()\n",
    "for t in datas:\n",
    "    if type(t['title'])!=str:\n",
    "        t['title']=' '.join(t['title'])\n",
    "    if type(t['abstract'])!=str:\n",
    "        t['abstract']=' '.join(t['abstract'])\n",
    "    s1s.append(t['title']+'.'+t['abstract'])\n",
    "    for gnd in t['dcterms:subject']:\n",
    "        if type(gnd)==dict:\n",
    "            gnd_id=gnd['@id']\n",
    "        else:\n",
    "            continue\n",
    "        cnt+=1\n",
    "        if gnd_id in gnd_id2name:\n",
    "            gnd_set.add(gnd_id)\n",
    "            gnd_val=gnd_id2name[gnd_id]\n",
    "            item={}\n",
    "            item['sentence2']= 'Name is '+gnd_val\n",
    "            item['sentence1']=t['title']+'.'+t['abstract']\n",
    "            item['label']=1\n",
    "            x_train.append(item)\n",
    "print(len(x_train))      \n",
    "print(cnt) \n",
    "print(len(s1s)) #输入句子有\n",
    "print(len(gnd_id2name.values()))   \n",
    "print(len(gnd_set)) #gnd_id有 25371个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5053' max='52850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5053/52850 45:14 < 7:08:05, 1.86 it/s, Epoch 0.19/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 11.75 GiB of which 137.44 MiB is free. Process 10247 has 16.33 MiB memory in use. Including non-PyTorch memory, this process has 11.21 GiB memory in use. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     16\u001b[0m args \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainingArguments(\n\u001b[1;32m     17\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/out\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# evaluation_strategy=\"epoch\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainer(\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     31\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     32\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     33\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/mpnet-base\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     38\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/mpnet-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:3715\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3713\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 3715\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/accelerate/accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 11.75 GiB of which 137.44 MiB is free. Process 10247 has 16.33 MiB memory in use. Including non-PyTorch memory, this process has 11.21 GiB memory in use. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "from datasets import Dataset\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4ac3093a84a825c3f9d1ebfeea07254f6412721e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "import torch\n",
    "# 清理缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = SentenceTransformer(\"models/mpnet-base\")\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [x[\"sentence1\"] for x in x_train],\n",
    "    \"positive\": [x[\"sentence2\"] for x in x_train],\n",
    "})\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"models/out\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_steps=20000,\n",
    "    warmup_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "if not os.path.exists('models/mpnet-base'):\n",
    "    os.makedirs('models/mpnet-base')\n",
    "model.save('models/mpnet-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa9d7ecfd0a4793881659b34e9fe03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  58%|#####7    | 52.4M/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae350fc6dc34b47abe9405ae859cdcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cb983e064c48c8a5a8f7771f336368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2132107ffd4565820ac4c2dc3a162d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90f970939534b439fb63054e107f095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a02b12af6fe4e3193671bea000a672a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44004' max='44004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44004/44004 47:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.709900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.623100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.616200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.594500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.550700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.526900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.496100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.526500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.504600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.349800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.325600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.332200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.214100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.224300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debc9cbb26634347937a2e9bab3ff573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\n",
    "from datasets import Dataset\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4ac3093a84a825c3f9d1ebfeea07254f6412721e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [x[\"sentence1\"] for x in x_train],\n",
    "    \"positive\": [x[\"sentence2\"] for x in x_train],\n",
    "})\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "if not os.path.exists('all-MiniLM-L6-v2'):\n",
    "    os.makedirs('all-MiniLM-L6-v2')\n",
    "model.save('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261edb1e25ce4d5d83bc0b65506a2b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf-mirror.com/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/eaa086f0ffee582aeb45b36e34cdd1fe2d6de2bef61f8a559a1bbc9bd955917b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1738045763&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODA0NTc2M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zZW50ZW5jZS10cmFuc2Zvcm1lcnMvcGFyYXBocmFzZS1tdWx0aWxpbmd1YWwtTWluaUxNLUwxMi12Mi9lYWEwODZmMGZmZWU1ODJhZWI0NWIzNmUzNGNkZDFmZTJkNmRlMmJlZjYxZjhhNTU5YTFiYmM5YmQ5NTU5MTdiP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=SoatNHSii88motbZ9AEsMBBZAlfvM-waEJKe2-6LL%7E1yWyECj2mlpzB2PO4ufGA92eNnWtz1BxaGfc3oL16AUJPUk2RXcTN5FICJc2dACHSGU-RYv6wEllyc8x-dm3Z4xA-vCbuJDJJNEXc1gQ%7EfVW8ceK3NXSM12XPLYK2BKTgyYdgSnkL%7EtwO3Ywqvo9X40ZAMvTjhv0UZlBxeH6xiQgdF4HXQY2GDO5KEduUUpfwRGOt9DTKtBL-kxLgkdShRsdBWs5jiMURijeVDfXi5qd6zLGw2kvUdtfmwDJxaDyVUrhyBG3gHv5tePkqV7kVjt75ceMiD6nGTadaVIoO9Aw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38726f77a56c4764927be9abbc5c4d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   2%|2         | 10.5M/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3396ad19471a4cb38a3711699ba39382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73adcacc7d1b46568cf4cdaab3e6cc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dba11618374995866c4064621573b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d839ae5104481faa42866206d45e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44004' max='44004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44004/44004 1:22:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.663200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.587400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.580700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.554100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.481200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.452300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.426600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.300300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.283400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.158000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871428d849d6420aace7b7e8f75cc6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\n",
    "from datasets import Dataset\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4ac3093a84a825c3f9d1ebfeea07254f6412721e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [x[\"sentence1\"] for x in x_train],\n",
    "    \"positive\": [x[\"sentence2\"] for x in x_train],\n",
    "})\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "save_path=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087f71a8ae26429c8e48314edea622aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  57%|#####7    | 252M/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b958a470ab41b9888d260b03d75def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ff1acda4214de0bd7b6ab7d7cfe6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d273c183094f37a81f4752fc3b2173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c81c918d53457397fd1da071cc684b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266953ea64884088a5f768a6ebf8dba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44004' max='44004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44004/44004 5:09:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.592300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.576700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.555700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.499800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.492600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.492300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.476900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>2.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>2.079500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0f61dc65af46a79f939e8dc2bc6602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\n",
    "from datasets import Dataset\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4ac3093a84a825c3f9d1ebfeea07254f6412721e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [x[\"sentence1\"] for x in x_train],\n",
    "    \"positive\": [x[\"sentence2\"] for x in x_train],\n",
    "})\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "save_path=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf2820d702d4ea9aff4f8a85f9b4a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  46%|####6     | 41.9M/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd63a76227f745a9a12356b6229eb237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e10a565323944528b9cf190a4df59a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89302d4652424d20bcfa2bf42774a582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfa4d00d4f94942bdac5fe97bd0713e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1c53b0421c4324ba56bc85ad213df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44004' max='44004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44004/44004 30:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.965500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.845600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.817900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.781200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.674000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.495100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.465500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.461300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.442400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.420500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.304200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.324900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.268400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed50f221f304024b211babca38cadd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\n",
    "from datasets import Dataset\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4ac3093a84a825c3f9d1ebfeea07254f6412721e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [x[\"sentence1\"] for x in x_train],\n",
    "    \"positive\": [x[\"sentence2\"] for x in x_train],\n",
    "})\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "save_path=\"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382bf6c156e84905886da29664f4efd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4faf72a6b54fc68c434f2e69323ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc1a3982ea04be292d8dd107b0f5df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c2d1a64ebf466b952a74450a8d0a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1845a707d45344bc8cbc09d31a54e73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/369 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f77c371ebf3443c8cb6818944e3b621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c738a1cc5fdc47a1a1a840e12ca6b25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecca3f403bc4c0da6713aeb124da58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674f783b367f4975a459679c53ea546f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f40db65954640e8bf86b4c3c7783412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34af7c96e7a9431eb89730933f39cb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94dbc2250895420ebccb26ce26e756d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44004' max='44004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44004/44004 3:20:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.644400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.578200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.504600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.454800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.437600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.429300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.274900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.275100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.271800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.281800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.190300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.166300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.159800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7d57a2ad30446791b5c12182e73e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\n",
    "from datasets import Dataset\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4ac3093a84a825c3f9d1ebfeea07254f6412721e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-distilroberta-v1\")\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [x[\"sentence1\"] for x in x_train],\n",
    "    \"positive\": [x[\"sentence2\"] for x in x_train],\n",
    "})\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "save_path=\"sentence-transformers/all-distilroberta-v1\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Code': 'gnd:4003694-7', 'Classification Number': '00', 'Classification Name': 'Unspezifische Allgemeinwörter', 'Name': 'Ausbreitung', 'Alternate Name': [], 'Related Subjects': [], 'Source': 'Du.'}\n",
      "gnd:1163295434 Airfix Marke\n",
      "len(train_datas)= 41902\n",
      "len(datas)= 41902\n",
      "{'title': 'Introduction to Mathematical Analysis', 'abstract': 'The book begins at an undergraduate student level, assuming only basic knowledge of calculus in one variable. It rigorously treats topics such as multivariable differential calculus, the Lebesgue integral, vector calculus and differential equations. After having created a solid foundation of topology and linear algebra, the text later expands into more advanced topics such as complex analysis, differential forms, calculus of variations, differential geometry and even functional analysis. Overall, this text provides a unique and well-rounded introduction to the highly developed and multi-faceted subject of mathematical analysis as understood by mathematicians today', 'dcterms:subject': {'@id': 'gnd:4001865-9'}, 'subject': ['Mathematics', 'Differential Equations', 'Matrix theory', 'Measure theory.', '(classificationName=linseach:mapping)mat', 'Functions of real variables.', '(classificationName=msc)54-01', '(classificationName=msc)15-01', '(classificationName=msc)53-01', '(classificationName=msc)*26-01', '(classificationName=loc)QA331.5', 'Algebra.', '(classificationName=rvk)SK 400', '(classificationName=bk, id=106423258)31.40 - Analysis: Allgemeines', 'Functions of complex variables', 'Sequences (Mathematics)', '(classificationName=ddc)515.8']}\n",
      "{'title': 'Mobile and handheld computing solutions for organizations and end-users', 'abstract': '\"This book discusses a broad range of topics in order to advance handheld knowledge and apply the proposed methods to real-world issues for organizations and end users\"--Provided by publisher', 'dcterms:subject': {'@id': 'gnd:7689895-7'}, 'subject': ['(classificationName=bk, id=106417665)53.74 - Hochfrequenztechnik, Funktechnik', 'Portable computers', '(classificationName=linseach:mapping)inf', 'Mobile computing', '(classificationName=ddc)004.165', '(classificationName=bk, id=106417967)54.80 - Angewandte Informatik', '(classificationName=loc)QA76.59', '(classificationName=linseach:mapping)elt']}\n",
      "运行时间: 1199.593193769455 秒\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "root='/media/jh/新加卷/2024_12_04/llms4subjects-main/shared-task-datasets/TIBKAT/'\n",
    "all_core='tib-core-subjects'\n",
    "def process_data(root,dev,en_de,datas):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'subject' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])!=str:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])!=str:\n",
    "            item['abstract']=' '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        item['dcterms:subject']=tmp['dcterms:subject']\n",
    "        item['subject']=tmp['subject']\n",
    "        datas.append(item)\n",
    "    for dirs in os.listdir(root+f'{all_core}/data/{dev}'):\n",
    "        for file in os.listdir(root+f'{all_core}/data/{dev}/{dirs}/{en_de}'):\n",
    "            get_train_data(root+f'{all_core}/data/{dev}/{dirs}/{en_de}/{file}')\n",
    "     \n",
    "train_datas = []\n",
    "en_de='en'\n",
    "dev='train'\n",
    "process_data(root,dev,en_de,train_datas)\n",
    "en_de='de'\n",
    "dev='train'\n",
    "process_data(root,dev,en_de,train_datas)\n",
    "random.shuffle(train_datas)\n",
    "\n",
    "# dev_datas=[]\n",
    "# en_de='en'\n",
    "# dev='dev'\n",
    "# process_data(root,dev,en_de,dev_datas)\n",
    "# en_de='de'\n",
    "# dev='dev'\n",
    "# process_data(root,dev,en_de,dev_datas)\n",
    "# random.shuffle(dev_datas)\n",
    "\n",
    "import json\n",
    "root='/media/jh/新加卷'\n",
    "file_path=f'{root}/2024_12_04/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json' #GND-Subjects-tib-core.json'\n",
    "with open(file_path,'r') as f:\n",
    "    data=json.loads(f.read())\n",
    "\n",
    "print(data[0])\n",
    "gnd_id2name={}\n",
    "for i in data:\n",
    "    gnd_id2name[i['Code']]=i['Name']\n",
    "    for j in i['Alternate Name']:\n",
    "        gnd_id2name[i['Code']]=gnd_id2name[i['Code']]+' '+j\n",
    "print(i['Code'],gnd_id2name[i['Code']])\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "print('len(train_datas)=',len(train_datas))\n",
    "# print('len(dev_datas)=',len(dev_datas))\n",
    "datas=train_datas\n",
    "print('len(datas)=',len(datas))\n",
    "# datas数据样本随机打乱\n",
    "import random\n",
    "print(datas[0])\n",
    "random.shuffle(datas)\n",
    "print(datas[0])\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "    y_t=pickle.load(f)\n",
    "\n",
    "model_name_or_path='models/gte_gnd' #\"gte-multilingual-base\"\n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)\n",
    "\n",
    "def process_data(datas,result):\n",
    "    batch_size=768\n",
    "    texts=[datas[i:i+batch_size] for i in range(0, len(datas), batch_size)]\n",
    "    if len(texts)*batch_size<len(datas):\n",
    "        texts.append([datas[len(texts)*batch_size:]])\n",
    "    for block in texts:\n",
    "        text=[t['title']+'.'+t['abstract'] for t in block]\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "        with torch.no_grad():\n",
    "            similarities=torch.tensor(similarities).to('cuda')\n",
    "            # print(similarities.shape) 上次保存的前500个\n",
    "            top_k = torch.topk(similarities, k=20, dim=0).indices.T.cpu().tolist() # 取出相似度最高的20个\n",
    "            #similarities中的元素取反\n",
    "            similarities=-similarities\n",
    "            top_k_last = torch.topk(similarities, k=20, dim=0).indices.T.cpu().tolist() # 取出相似度最低的20个\n",
    "        for i,t in enumerate(block):\n",
    "            pos=[]\n",
    "            for gnd in t['dcterms:subject']:\n",
    "                if type(gnd)==dict:\n",
    "                    gnd_id=gnd['@id']\n",
    "                else:\n",
    "                    continue\n",
    "                if gnd_id in gnd_id2name:\n",
    "                    pos.append(gnd_id2name[gnd_id])\n",
    "            for j in top_k[i]:\n",
    "                pos.append(gnd_id2name[y_t[j]['Code']])\n",
    "            item1={}\n",
    "            item1['pos']='.'.join(pos)\n",
    "            item1['neg']='. '.join([gnd_id2name[y_t[j]['Code']] for j in top_k_last[i]])\n",
    "            item1['text']=t['title']+'.'+t['abstract']\n",
    "            result.append(item1)\n",
    "\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "x_train=[]\n",
    "process_data(datas,x_train) \n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 21:20:46.848702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737206446.863584   65100 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737206446.867164   65100 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-18 21:20:46.882469: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27654' max='27654' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27654/27654 2:10:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.122100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>5.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>5.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.999900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>4.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>4.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>5.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>5.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>5.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>4.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>5.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>5.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>4.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>4.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>5.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>5.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>5.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>4.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>5.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>4.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>5.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>5.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>5.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.998500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>2.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2561049a62d14d818805ab97a0306d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4ac3093a84a825c3f9d1ebfeea07254f6412721e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "import pickle\n",
    "\n",
    "with open('./tmp/gnd_all_train.pkl','rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "\n",
    "model_name_or_path='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "model = SentenceTransformer(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "# # 冻结前10层参数\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'layer.11' not in name and 'layer.10' not in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir='./out',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # 减小批量大小\n",
    "    per_device_eval_batch_size=8,  # 减小批量大小\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # 混合精度训练\n",
    "    bf16=False,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=1000,\n",
    "    # gradient_accumulation_steps=2,  # 梯度累积\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [t['text'] for t in x_train],\n",
    "    \"positive\": [t['pos'] for t in x_train],\n",
    "    \"negative\": [t['neg'] for t in x_train],\n",
    "})\n",
    "\n",
    "train_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "loss = losses.TripletLoss(model=model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    # args=args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    eval_dataset=train_dataset['test'],\n",
    "    loss=loss,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save('models/mpnet_gnd_triplet') # 运行174m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 清除GPU缓存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Code': 'gnd:4003694-7', 'Classification Number': '00', 'Classification Name': 'Unspezifische Allgemeinwörter', 'Name': 'Ausbreitung', 'Alternate Name': [], 'Related Subjects': [], 'Source': 'Du.'}\n",
      "gnd:1163295434 Airfix Marke\n",
      "len(train_datas)= 81937\n",
      "len(datas)= 81937\n",
      "{'title': 'Evolution des Fahrrads', 'abstract': 'Das Fahrrad ist das langlebigste und weitest verbreite Fahrzeug der Welt, von dem in seiner zweihundertjährigen Geschichte mehr als eine Milliarde Exemplare gebaut wurden. Dennoch wurde seine Technikgeschichte bisher vernachlässigt, obwohl es mit seiner Idee des Leichtbaus anstelle der tonnenschweren Eisenbahn und der Abkehr vom Pferd neue Konstruktionen, Materialien und Fertigungstechniken ins Leben rief, welche die Basis für Motorrad-, Automobil- und Flugzeugbau bildeten. Erst seit dreißig Jahren haben eine internationale Konferenz und die mikrohistorische Feinarbeit der nationalen Vintage-Vereine quellenbasierte Erkenntnisse gesammelt, die einen neuen Blick auf die Fahrradgeschichte ermöglichen. Dieses Buch bietet erstmals eine länderübergreifende und umfassende Darstellung der technischen und historischen Entwicklung des Fahrrads von den frühesten Laufmaschinen als Reitpferdersatz bis hin zu modernen Rennrädern, Mountainbikes und Liegerädern. Es zeichnet die Entwicklung des Fahrrads auch anhand der einzelnen Komponenten wie Antrieben, Schaltungen, Federungen, Bremsen, Lichtanlagen, Speichenrädern und Luftreifen nach, die von Erfindern und Herstellern hinsichtlich Material, Ergonomie und Fahrphysik zuwege gebracht wurden. Mit vielen Original- und Patentgrafiken sowie dem ausgewählten Literaturverzeichnis stellt dieses Buch den Ausgangspunkt für alle künftigen Recherchen dar. Hans-Erhard Lessing wirkte an Museen in Mannheim und Karlsruhe als Hauptkonservator und ist apl. Professor der Universität Ulm. Er schrieb Biographien von Robert Bosch und Karl Drais sowie weitere technikhistorische Bücher. Tony Hadland wirkte jahrzehntelang als Baugutachter bei Barclays Bank. Er ist Autor der Firmengeschichten von Raleigh, Sturmey-Archer und Moulton, sowie weiterer technikhistorischer Bücher und hat einen eigenen Blog. Velozipede und ihre Vorgänger -- Eins der beiden Räder antreiben -- Drahtspeichenräder -- Indirekter Antrieb -- Das Niederrad -- Komfort -- Den Antrieb verbessern -- Bremsen -- Sättel, Pedale und Lenker -- Beleuchtung -- Gepäck -- Rennräder -- Militärfahrräder -- Mountainbikes -- Fahrräder mit kleinen Rädern -- Liegefahrräder. .', 'dcterms:subject': [{'@id': 'gnd:4153568-6'}, {'@id': 'gnd:4011510-0'}, {'@id': 'gnd:4032231-2'}, {'@id': 'gnd:4016297-7'}], 'subject': ['Engineering design.', 'Mechanical engineering.', '(classificationName=linseach:mapping)ver', 'Geschichte', '(classificationName=rvk)ZO 4340', '(classificationName=ddc)621']}\n",
      "{'title': 'Revolution, Fortschritt und Verfassung : zu einem neuen Verfassungsverständnis', 'abstract': 'Ulrich K. Preuß: \"Revolution, Fortschritt und Verfassung\". Zu einem neuen Verfassungsverständnis, Wagenbach Verlag, Berlin 1990. 100 S., br., DM 23,-', 'dcterms:subject': [{'@id': 'gnd:4049680-6'}, {'@id': 'gnd:4062787-1'}], 'subject': ['Fortschritt, sozialer', 'History', '(classificationName=bk, id=10642260X)86.44 - Staatsrecht, Verfassungsrecht: Allgemeines', '(classificationName=linseach:mapping)jur', '(classificationName=ssg)3,6', '(classificationName=loksys-fbr)H 320', '(classificationName=loksys-fbr)H 310', 'Verfassungsstaat. Theorien', 'Staatsrecht und Verfassungsrecht', '(classificationName=rvk)MD 8300', '(classificationName=loc)JF1051', 'Verfassung', 'Revolutions', 'Rechtsphilosophie', '(classificationName=loksys-fbr)jur 132.1 ea', '(classificationName=dbn)16', '(classificationName=ddc)323/.042', 'Representative government and representation', 'Verfassungsgeschichte', '(classificationName=rvk)ME 4000']}\n",
      "运行时间: 1717.0430612564087 秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 清除GPU缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "root='/media/jh/新加卷/2024_12_04/llms4subjects-main/shared-task-datasets/TIBKAT/'\n",
    "all_core='all-subjects'\n",
    "def process_data(root,dev,en_de,datas):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'subject' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])!=str:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])!=str:\n",
    "            item['abstract']=' '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        item['dcterms:subject']=tmp['dcterms:subject']\n",
    "        item['subject']=tmp['subject']\n",
    "        datas.append(item)\n",
    "    for dirs in os.listdir(root+f'{all_core}/data/{dev}'):\n",
    "        for file in os.listdir(root+f'{all_core}/data/{dev}/{dirs}/{en_de}'):\n",
    "            get_train_data(root+f'{all_core}/data/{dev}/{dirs}/{en_de}/{file}')\n",
    "     \n",
    "train_datas = []\n",
    "en_de='en'\n",
    "dev='train'\n",
    "process_data(root,dev,en_de,train_datas)\n",
    "en_de='de'\n",
    "dev='train'\n",
    "process_data(root,dev,en_de,train_datas)\n",
    "random.shuffle(train_datas)\n",
    "\n",
    "# dev_datas=[]\n",
    "# en_de='en'\n",
    "# dev='dev'\n",
    "# process_data(root,dev,en_de,dev_datas)\n",
    "# en_de='de'\n",
    "# dev='dev'\n",
    "# process_data(root,dev,en_de,dev_datas)\n",
    "# random.shuffle(dev_datas)\n",
    "\n",
    "import json\n",
    "root='/media/jh/新加卷'\n",
    "file_path=f'{root}/2024_12_04/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json' #GND-Subjects-tib-core.json'\n",
    "with open(file_path,'r') as f:\n",
    "    data=json.loads(f.read())\n",
    "\n",
    "print(data[0])\n",
    "gnd_id2name={}\n",
    "for i in data:\n",
    "    gnd_id2name[i['Code']]=i['Name']\n",
    "    for j in i['Alternate Name']:\n",
    "        gnd_id2name[i['Code']]=gnd_id2name[i['Code']]+' '+j\n",
    "print(i['Code'],gnd_id2name[i['Code']])\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "print('len(train_datas)=',len(train_datas))\n",
    "# print('len(dev_datas)=',len(dev_datas))\n",
    "datas=train_datas\n",
    "print('len(datas)=',len(datas))\n",
    "# datas数据样本随机打乱\n",
    "import random\n",
    "print(datas[0])\n",
    "random.shuffle(datas)\n",
    "print(datas[0])\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "with open('./tmp/core_labels_mpnet.pkl','rb') as f:\n",
    "    y_t=pickle.load(f)\n",
    "\n",
    "model_name_or_path='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'#\"gte-multilingual-base\"\n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)\n",
    "\n",
    "def process_data(datas,result):\n",
    "    batch_size=768\n",
    "    texts=[datas[i:i+batch_size] for i in range(0, len(datas), batch_size)]\n",
    "    if len(texts)*batch_size<len(datas):\n",
    "        texts.append([datas[len(texts)*batch_size:]])\n",
    "    for block in texts:\n",
    "        text=[t['title']+'.'+t['abstract'] for t in block]\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "        with torch.no_grad():\n",
    "            similarities=torch.tensor(similarities).to('cuda')\n",
    "            # print(similarities.shape) 上次保存的前500个\n",
    "            top_k = torch.topk(similarities, k=20, dim=0).indices.T.cpu().tolist() # 取出相似度最高的20个\n",
    "            #similarities中的元素取反\n",
    "            similarities=-similarities\n",
    "            top_k_last = torch.topk(similarities, k=20, dim=0).indices.T.cpu().tolist() # 取出相似度最低的20个\n",
    "        for i,t in enumerate(block):\n",
    "            pos=[]\n",
    "            for gnd in t['dcterms:subject']:\n",
    "                if type(gnd)==dict:\n",
    "                    gnd_id=gnd['@id']\n",
    "                else:\n",
    "                    continue\n",
    "                if gnd_id in gnd_id2name:\n",
    "                    pos.append(gnd_id2name[gnd_id])\n",
    "            for j in top_k[i]:\n",
    "                pos.append(gnd_id2name[y_t[j]['Code']])\n",
    "            item1={}\n",
    "            item1['pos']='.'.join(pos)\n",
    "            item1['neg']='. '.join([gnd_id2name[y_t[j]['Code']] for j in top_k_last[i]])\n",
    "            item1['text']=t['title']+'.'+t['abstract']\n",
    "            result.append(item1)\n",
    "\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "x_train=[]\n",
    "process_data(datas,x_train) \n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')\n",
    "# 保存训练数据\n",
    "with open('./tmp/gnd_all_train.pkl','wb') as f:\n",
    "    pickle.dump(x_train,f)\n",
    "import torch\n",
    "# 清除GPU缓存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 清除GPU缓存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 20:51:05.085841: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737204665.102990   63283 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737204665.108285   63283 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-18 20:51:05.127097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2941' max='55308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2941/55308 25:22 < 7:32:14, 1.93 it/s, Epoch 0.16/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7a50f07ccf4615ac3b6408713e867f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 64\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mContrastiveLoss(model)\n\u001b[1;32m     56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainer(\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# args=args,\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 64\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m model_out_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/mpnet_gte_gnd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_out_dir):\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:2536\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4ac3093a84a825c3f9d1ebfeea07254f6412721e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "model_name_or_path = '/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/' #\"gte-multilingual-base\"\n",
    "model = SentenceTransformer(model_name_or_path)\n",
    "# 冻结部分参数: 训练过程中出现loss为0的情况。可能是模型过于复杂。这里我们冻结部分参数，只训练最后一层。\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'encoder' in name:\n",
    "#         param.requires_grad = False\n",
    "#     if 'layer.11' in name or 'layer.10' in name:\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# from datasets import Dataset\n",
    "# import pickle\n",
    "# with open('./tmp/gnd_all_train.pkl','rb') as f:\n",
    "#     x_train = pickle.load(f)\n",
    "# train_dataset = Dataset.from_dict({\n",
    "#     \"snentence1\": [t['text'] for t in x_train]*2,\n",
    "#     \"sentence2\": [t['pos'] for t in x_train]+[t['neg'] for t in x_train],\n",
    "#     \"label\": [1]*len(x_train)+[0]*len(x_train),\n",
    "# })\n",
    "from datasets import Dataset\n",
    "import pickle\n",
    "with open('./tmp/gnd_all_train.pkl','rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "data_tmp=[]\n",
    "for t in x_train:\n",
    "    data_tmp.append({'snentence1':t['text'],'sentence2':t['pos'],'label':1})\n",
    "    data_tmp.append({'snentence1':t['text'],'sentence2':t['neg'],'label':0})\n",
    "# 打乱数据集\n",
    "import random\n",
    "random.shuffle(data_tmp)\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"snentence1\": [t['snentence1'] for t in data_tmp],\n",
    "    \"sentence2\": [t['sentence2'] for t in data_tmp],\n",
    "    \"label\": [t['label'] for t in data_tmp],\n",
    "})\n",
    "\n",
    "# 打乱数据集\n",
    "train_dataset = train_dataset.shuffle()\n",
    "train_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "loss = losses.ContrastiveLoss(model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    # args=args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    eval_dataset=train_dataset['test'],\n",
    "    loss=loss,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model_out_dir = 'models/mpnet_gte_gnd'\n",
    "if not os.path.exists(model_out_dir):\n",
    "    os.makedirs(model_out_dir)\n",
    "model.save(model_out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.auto_model.embeddings.word_embeddings.weight True\n",
      "0.auto_model.embeddings.position_embeddings.weight True\n",
      "0.auto_model.embeddings.token_type_embeddings.weight True\n",
      "0.auto_model.embeddings.LayerNorm.weight True\n",
      "0.auto_model.embeddings.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.0.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.0.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.0.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.0.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.0.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.0.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.0.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.0.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.0.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.0.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.0.output.dense.weight True\n",
      "0.auto_model.encoder.layer.0.output.dense.bias True\n",
      "0.auto_model.encoder.layer.0.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.0.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.1.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.1.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.1.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.1.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.1.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.1.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.1.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.1.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.1.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.1.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.1.output.dense.weight True\n",
      "0.auto_model.encoder.layer.1.output.dense.bias True\n",
      "0.auto_model.encoder.layer.1.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.1.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.2.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.2.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.2.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.2.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.2.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.2.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.2.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.2.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.2.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.2.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.2.output.dense.weight True\n",
      "0.auto_model.encoder.layer.2.output.dense.bias True\n",
      "0.auto_model.encoder.layer.2.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.2.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.3.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.3.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.3.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.3.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.3.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.3.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.3.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.3.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.3.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.3.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.3.output.dense.weight True\n",
      "0.auto_model.encoder.layer.3.output.dense.bias True\n",
      "0.auto_model.encoder.layer.3.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.3.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.4.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.4.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.4.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.4.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.4.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.4.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.4.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.4.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.4.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.4.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.4.output.dense.weight True\n",
      "0.auto_model.encoder.layer.4.output.dense.bias True\n",
      "0.auto_model.encoder.layer.4.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.4.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.5.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.5.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.5.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.5.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.5.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.5.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.5.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.5.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.5.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.5.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.5.output.dense.weight True\n",
      "0.auto_model.encoder.layer.5.output.dense.bias True\n",
      "0.auto_model.encoder.layer.5.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.5.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.6.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.6.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.6.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.6.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.6.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.6.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.6.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.6.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.6.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.6.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.6.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.6.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.6.output.dense.weight True\n",
      "0.auto_model.encoder.layer.6.output.dense.bias True\n",
      "0.auto_model.encoder.layer.6.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.6.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.7.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.7.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.7.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.7.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.7.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.7.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.7.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.7.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.7.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.7.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.7.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.7.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.7.output.dense.weight True\n",
      "0.auto_model.encoder.layer.7.output.dense.bias True\n",
      "0.auto_model.encoder.layer.7.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.7.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.8.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.8.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.8.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.8.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.8.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.8.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.8.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.8.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.8.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.8.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.8.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.8.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.8.output.dense.weight True\n",
      "0.auto_model.encoder.layer.8.output.dense.bias True\n",
      "0.auto_model.encoder.layer.8.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.8.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.9.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.9.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.9.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.9.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.9.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.9.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.9.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.9.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.9.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.9.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.9.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.9.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.9.output.dense.weight True\n",
      "0.auto_model.encoder.layer.9.output.dense.bias True\n",
      "0.auto_model.encoder.layer.9.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.9.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.10.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.10.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.10.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.10.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.10.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.10.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.10.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.10.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.10.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.10.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.10.output.dense.weight True\n",
      "0.auto_model.encoder.layer.10.output.dense.bias True\n",
      "0.auto_model.encoder.layer.10.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.10.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.11.attention.self.query.weight True\n",
      "0.auto_model.encoder.layer.11.attention.self.query.bias True\n",
      "0.auto_model.encoder.layer.11.attention.self.key.weight True\n",
      "0.auto_model.encoder.layer.11.attention.self.key.bias True\n",
      "0.auto_model.encoder.layer.11.attention.self.value.weight True\n",
      "0.auto_model.encoder.layer.11.attention.self.value.bias True\n",
      "0.auto_model.encoder.layer.11.attention.output.dense.weight True\n",
      "0.auto_model.encoder.layer.11.attention.output.dense.bias True\n",
      "0.auto_model.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "0.auto_model.encoder.layer.11.intermediate.dense.weight True\n",
      "0.auto_model.encoder.layer.11.intermediate.dense.bias True\n",
      "0.auto_model.encoder.layer.11.output.dense.weight True\n",
      "0.auto_model.encoder.layer.11.output.dense.bias True\n",
      "0.auto_model.encoder.layer.11.output.LayerNorm.weight True\n",
      "0.auto_model.encoder.layer.11.output.LayerNorm.bias True\n",
      "0.auto_model.pooler.dense.weight True\n",
      "0.auto_model.pooler.dense.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gte-multilingual-base were not used when initializing NewModel: {'classifier.bias', 'classifier.weight'}\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44004' max='44004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44004/44004 1:45:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.344800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.336800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.301400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.306100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.300500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.293500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.299500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.269100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.267200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.253700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.235900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.213700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724a9661f1774352909795339f9894a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=44004, training_loss=0.273337655819857, metrics={'train_runtime': 6348.5474, 'train_samples_per_second': 55.448, 'train_steps_per_second': 6.931, 'total_flos': 0.0, 'train_loss': 0.273337655819857, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "from datasets import Dataset\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4ac3093a84a825c3f9d1ebfeea07254f6412721e\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "anchor = [x['sentence1'] for x in x_train]\n",
    "positive = [x['sentence2'] for x in x_train]\n",
    "# model_id='sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name_or_path=\"gte-multilingual-base\"\n",
    "model = SentenceTransformer(model_name_or_path, trust_remote_code=True)\n",
    "model=model.to('cuda')\n",
    "# 冻结前10层参数\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layer.11' not in name:\n",
    "        param.requires_grad = False\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": anchor,\n",
    "    \"positive\": positive,\n",
    "})\n",
    "out_dir='/media/jh/新加卷/2024_12_04/gnd_models'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "# 5. (Optional) Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=out_dir, #\"models/mpnet-base-all-nli-triplet\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=160,#16,\n",
    "    per_device_eval_batch_size=160,#16,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True, # Set to False if GPU can't handle FP16\n",
    "    bf16=False, # Set to True if GPU supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES, # MultipleNegativesRankingLoss benefits from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=1000,\n",
    ")\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    \n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "# 6. Save the model\n",
    "\n",
    "# model.save(out_dir) # core 181m, all: 274m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载检查点\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('/media/jh/新加卷/2025_1_16/tmp_trainer/checkpoint-44004/', trust_remote_code=True)\n",
    "model.save('./models/gte_gnd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
