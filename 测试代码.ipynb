{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0033, -0.0886, -0.0190,  ...,  0.0074, -0.0579, -0.0142],\n",
      "        [-0.0396, -0.0846, -0.0084,  ...,  0.0313, -0.0635, -0.0269],\n",
      "        [-0.0291, -0.0442,  0.0145,  ...,  0.0172, -0.0733, -0.0210]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "204739 204739\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6045\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7621\n",
      "运行时间: 850.9448997974396 秒\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "root='/media/jh/新加卷1'\n",
    "with open(f'{root}/2024_11_10/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# model_id='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "model_name_or_path='out_models/all-mpnet-base-v2'#'/media/jh/新加卷/2025_1_16/sentence-transformers/all-mpnet-base-v2/' #models/gte_gnd 可以 \n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)#, \n",
    "# model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "def process_data(en_de,root,des_root,mode='m'):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        if not os.path.exists(f'{root}/{dirs}/{en_de}'):\n",
    "            continue\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            # if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "            #     continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            # 把file的后缀名去掉\n",
    "            file=file.split('.')[0]+'.json'\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    batch_size=768\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    index1=0\n",
    "    \n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "            # print(torch.tensor(similarities).shape) #(n,768)\n",
    "        if mode=='m2':\n",
    "            m2_similarities.extend(torch.tensor(similarities).T.tolist())\n",
    "            print(torch.tensor(m2_similarities).shape)\n",
    "            continue\n",
    "        elif mode=='m1':\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=500, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                sims=[]\n",
    "                for i in t:\n",
    "                    sims.append(similarities[index1][i])\n",
    "                index1+=1\n",
    "                top_k_2=torch.topk(torch.tensor(sims), k=50, dim=0).indices.T.cpu().tolist()\n",
    "                code=[]\n",
    "                for i in top_k_2:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=50, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                code=[]\n",
    "                for i in t:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            # if os.path.exists(file_names[index]):\n",
    "            #     index+=1\n",
    "            #     continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "        print('batch',index)\n",
    "        result=[] #重置result\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "root1='/media/jh/新加卷1'\n",
    "root='/media/jh/新加卷1/2024_11_10/llms4subjects-main/shared-task-datasets/TIBKAT/all-subjects/data/dev/' #测试数据集目录\n",
    "des_root='/media/jh/新加卷1/2024_12_04/2025_1/test2' #生成的数据的目标目录\n",
    "# model_id='./models/mpnet_gnd_triplet'#'/media/jh/新加卷/2025_1_16/final_model'\n",
    "# # with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "# #     y_t=pickle.load(f)\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "\n",
    "# m2_similarities=[]\n",
    "process_data('en',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_en.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "# m2_similarities=[]\n",
    "process_data('de',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_de.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6372,  0.2573, -0.7280,  ..., -0.5933, -0.0065,  0.7397],\n",
      "        [-0.6592,  0.2507, -0.7104,  ..., -0.5747,  0.0039,  0.7622],\n",
      "        [-0.6904,  0.2585, -0.7319,  ..., -0.5820,  0.0142,  0.8120]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "204739 204739\n",
      "x_test: 7621\n",
      "x_test替换成功!\n",
      "x_test: 7621\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7621\n",
      "运行时间: 320.1072027683258 秒\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "root='/media/jh/新加卷1'\n",
    "with open(f'{root}/2024_11_10/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# model_id='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "model_name_or_path='tmp/out_models/paraphrase-MiniLM-L6-v2'#'/media/jh/新加卷/2025_1_16/sentence-transformers/all-mpnet-base-v2/' #models/gte_gnd 可以 \n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)#, \n",
    "# model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=64\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "def process_data(en_de,root,des_root,mode='m'):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        if not os.path.exists(f'{root}/{dirs}/{en_de}'):\n",
    "            continue\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            # if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "            #     continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            # 把file的后缀名去掉\n",
    "            file=file.split('.')[0]+'.json'\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    print('x_test:',len(x_test))\n",
    "    if en_de=='de':\n",
    "        with open('tmp/x_train_de2en.json','r') as f:\n",
    "            x_train=json.load(f)\n",
    "        x_test=[x['sentence1'] for x in x_train if x['label']!=0]\n",
    "        # 德语翻译成英语后进行测试。x_train_de2en.json存放的是all-subjects_dev_de.json经过翻译后的数据\n",
    "        print('x_test替换成功!')\n",
    "    print('x_test:',len(x_test))\n",
    "    batch_size=768\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    index1=0\n",
    "    \n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "            # print(torch.tensor(similarities).shape) #(n,768)\n",
    "        if mode=='m2':\n",
    "            m2_similarities.extend(torch.tensor(similarities).T.tolist())\n",
    "            print(torch.tensor(m2_similarities).shape)\n",
    "            continue\n",
    "        elif mode=='m1':\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=500, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                sims=[]\n",
    "                for i in t:\n",
    "                    sims.append(similarities[index1][i])\n",
    "                index1+=1\n",
    "                top_k_2=torch.topk(torch.tensor(sims), k=50, dim=0).indices.T.cpu().tolist()\n",
    "                code=[]\n",
    "                for i in top_k_2:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=50, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                code=[]\n",
    "                for i in t:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            # if os.path.exists(file_names[index]):\n",
    "            #     index+=1\n",
    "            #     continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "        print('batch',index)\n",
    "        result=[] #重置result\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "root1='/media/jh/新加卷1'\n",
    "root='/media/jh/新加卷1/2024_11_10/llms4subjects-main/shared-task-datasets/TIBKAT/all-subjects/data/dev/'\n",
    "des_root='/media/jh/新加卷1/2024_12_04/2025/test3-5'\n",
    "# model_id='./models/mpnet_gnd_triplet'#'/media/jh/新加卷/2025_1_16/final_model'\n",
    "# # with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "# #     y_t=pickle.load(f)\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "\n",
    "# m2_similarities=[]\n",
    "# process_data('en',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_en.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "# m2_similarities=[]\n",
    "process_data('de',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_de.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
