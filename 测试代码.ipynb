{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 15:08:02.414617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737270482.426547   95577 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737270482.429982   95577 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-19 15:08:02.443832: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0257,  0.1114, -0.0401,  ..., -0.1732, -0.1499, -0.4778],\n",
      "        [ 0.0266,  0.1118, -0.0426,  ..., -0.1720, -0.1483, -0.4785],\n",
      "        [ 0.0261,  0.1116, -0.0411,  ..., -0.1726, -0.1490, -0.4780]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "79427 79427\n",
      "2025-01-19 15:08:35.115067\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "root='/media/jh/新加卷'\n",
    "with open(f'{root}/2024_12_04/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-tib-core.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# model_id='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "model_name_or_path='models/mpnet_gnd_triplet' #\"gte-multilingual-base\"\n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)\n",
    "# model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "# import pickle\n",
    "# with open('./tmp/core_labels_mpnet.pkl','wb') as f:\n",
    "#     pickle.dump(y_t,f)\n",
    "\n",
    "# 打印当前时间\n",
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Code': 'gnd:4003694-7', 'Classification Number': '00', 'Classification Name': 'Unspezifische Allgemeinwörter', 'Name': 'Ausbreitung', 'Alternate Name': [], 'Related Subjects': [], 'Source': 'Du.'}\n",
      "gnd:1163295434 Airfix Marke\n",
      "len(train_datas)= 41902\n",
      "len(datas)= 41902\n",
      "{'title': 'Jumpstart Tableau : A Step-By-Step Guide to Better Data Visualization', 'abstract': 'Chapter 1: Logon to Tableau -- Chapter 2: Connecting to Two Data Sources -- Chapter 3: Exploring the Tableau Interface -- Chapter 4: Developing a Simple Visualization -- Chapter 5: Saving Tableau Workbook and Tableau Packaged Workbook -- Chapter 6: Using Basic Analysis Functions -- Chapter 7: Adding, Removing, and Renaming a Dimension -- Chapter 8: Copying or Deleting a Worksheet -- Chapter 9: Changing the Display from One Chart Type to Another -- Chapter 10: Using the Show Me Tool for Selecting a Visualization -- Chapter 11: Crosstab Display and Swapping -- Chapter 12: Sorting -- Chapter 13: More about Sorting -- Chapter 14: View Detail/Underlying Data -- Chapter 15: Grouping -- Chapter 16: Building a Hierarchy -- Chapter 17: Aggregate Measures -- Chapter 18: Exclude and Keep -- Chapter 19: Filtering on the Filter Shelf -- Chapter 20: Quick Filters -- Chapter 21: Customization for Quick Filters -- Chapter 22: Quick Filters Single and Multiple Value Lists -- Chapter 23: Quick Filters Sliders -- Chapter 24: Dependency in Quick Filters -- Chapter 25: Saving in PDF Format -- Chapter 26: Exporting an Image to PowerPoint -- Chapter 27: Exporting Data -- Chapter 28: Displaying Underlying Data -- Chapter 29: Exporting Crosstab Data -- Chapter 30: Formatting -- Chapter 31: Highlighting with Colors -- Chapter 32: Axis Formatting -- Chapter 33: Formatting Tables -- Chapter 34: Top N Function -- Chapter 35: Trendlines -- Chapter 36: Forecasting -- Chapter 37: Creating a Dashboard -- Chapter 38: Dashboard Quick Filters -- Chapter 39: Cascading Worksheet Changes in a Dashboard -- Chapter 40: Dashboard Layout Formatting. . This book simplifies the use of Tableau software functionality for novice users so that they can create powerful data visualizations easily and quickly. Since it is often very difficult and expensive to provide external training on BI tools, this book aims to equip the reader with the resource they need to do it themselves. Jumpstart Tableau covers the basic reporting and analysis functions that most BI users perform in their day-to-day work. These include connecting to a data source, working with dimensions and measures, developing reports and charts, saving workbooks, filtering, swapping, sorting, formatting, grouping, creating hierarchies, forecasting, exporting, distributing, as well developing various chart types. Each exercise in Jumpstart Tableau provides screenshots that cover every step from start to finish. The exercises are based on a comprehensive sample Excel-based data source that Tableau Software (version 9) has provided, which makes it very easy to duplicate the exercises on the real software. In addition, the book: Enables readers to develop reports, queries, and visualizations Perform data analysis Execute each function in a step-by-step manner Provides the basic hands-on ability which can enable users to work up to more advanced and complex Tableau functionality Shows how to integrate individual development of content, such as tables/charts and visualizations., onto a dashboard for an effective presentation.', 'dcterms:subject': [{'@id': 'gnd:4428654-5'}, {'@id': 'gnd:1036568350'}, {'@id': 'gnd:4213132-7'}, {'@id': 'gnd:4546354-2'}], 'subject': ['Software engineering', 'Management information systems', 'Business and Management', '(classificationName=linseach:mapping)inf', '(classificationName=rvk)ST 601', 'Business analytics', 'Dashboards', '(classificationName=rvk)ST 331', 'Business', 'Data dimensions', '(classificationName=ddc)004', '(classificationName=loc)HD30.2', 'Data Visualization', 'Data analytics', 'Tableau Software', 'Data sources', 'Application software.', 'Data reporting', 'Business Intelligence', '(classificationName=ddc)658.4038', 'Data measures']}\n",
      "{'title': 'Modellbildung und Simulation : Mit einer Einführung in ANSYS', 'abstract': 'Einführung -- Modellierung und Simulationen mit finiten Differenzenverfahren -- Randangepasste Gitter -- Finite-Elemente-Methode für eindimensionale Probleme -- Finite-Elemente-Methode bei elliptischen Randwertproblemen -- Einführung in ANSYS -- ANSYS-Simulationen -Projektarbeiten. Das Buch vermittelt leicht fasslich ein mathematisches Verständnis für die modernen Simulationsmethoden. Es befähigt, Simulationsergebnisse kritisch zu beurteilen. Dazu ist es erforderlich, die typischen Fehlerquellen zu kennen, die bei den eingesetzten Methoden auftreten können. Die vorgestellten Methoden bilden die Grundlage für fast alle gängigen Softwaretools. Neben der Modell- und Methodenbeschreibung demonstriert der Autor, wie sich grundsätzlich mit einem Computerprogramm eine Vielfalt unterschiedlicher Problemstellungen lösen lassen. Für die Durchführung der Simulation am Rechner wird das weltweit verbreitete Finite-Elemente-Programm ANSYS verwendet.', 'dcterms:subject': [{'@id': 'gnd:4211838-4'}, {'@id': 'gnd:4148259-1'}, {'@id': 'gnd:4194626-1'}, {'@id': 'gnd:4126276-1'}, {'@id': 'gnd:4017233-8'}, {'@id': 'gnd:4114528-8'}], 'subject': ['(classificationName=loc)TA329-348', '(classificationName=msc)00A72', '(classificationName=rvk)SK 910', '(classificationName=ddc)519', '(classificationName=rvk)ST 620', '(classificationName=msc)65N30', 'Engineering', 'ANSYS (Computer system)', '(classificationName=msc)00A71', 'Applied mathematics.', 'Mathematical models.', 'Robotics.', '(classificationName=loc)TA640-643', 'Control engineering.', '(classificationName=msc)*00A06', '(classificationName=msc)65M60', 'Mechatronics.', '(classificationName=msc)97M10', 'Control engineering systems', '(classificationName=ddc)620.00113', '(classificationName=rvk)ST 340', 'Engineering mathematics', '(classificationName=bk, id=181571455)50.03 - Methoden und Techniken der Ingenieurwissenschaften', '(classificationName=linseach:mapping)mat', '(classificationName=loc)TA345.5.A57', '(classificationName=linseach:mapping)tec', 'Finite element method']}\n",
      "运行时间: 975.3658411502838 秒\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "root='/media/jh/新加卷/2024_12_04/llms4subjects-main/shared-task-datasets/TIBKAT/'\n",
    "all_core='tib-core-subjects'\n",
    "def process_data(root,dev,en_de,datas):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'subject' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])!=str:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])!=str:\n",
    "            item['abstract']=' '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        item['dcterms:subject']=tmp['dcterms:subject']\n",
    "        item['subject']=tmp['subject']\n",
    "        datas.append(item)\n",
    "    for dirs in os.listdir(root+f'{all_core}/data/{dev}'):\n",
    "        for file in os.listdir(root+f'{all_core}/data/{dev}/{dirs}/{en_de}'):\n",
    "            get_train_data(root+f'{all_core}/data/{dev}/{dirs}/{en_de}/{file}')\n",
    "     \n",
    "train_datas = []\n",
    "en_de='en'\n",
    "dev='train'\n",
    "process_data(root,dev,en_de,train_datas)\n",
    "en_de='de'\n",
    "dev='train'\n",
    "process_data(root,dev,en_de,train_datas)\n",
    "random.shuffle(train_datas)\n",
    "\n",
    "# dev_datas=[]\n",
    "# en_de='en'\n",
    "# dev='dev'\n",
    "# process_data(root,dev,en_de,dev_datas)\n",
    "# en_de='de'\n",
    "# dev='dev'\n",
    "# process_data(root,dev,en_de,dev_datas)\n",
    "# random.shuffle(dev_datas)\n",
    "\n",
    "import json\n",
    "root='/media/jh/新加卷'\n",
    "file_path=f'{root}/2024_12_04/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json' #GND-Subjects-tib-core.json'\n",
    "with open(file_path,'r') as f:\n",
    "    data=json.loads(f.read())\n",
    "\n",
    "print(data[0])\n",
    "gnd_id2name={}\n",
    "for i in data:\n",
    "    gnd_id2name[i['Code']]=i['Name']\n",
    "    for j in i['Alternate Name']:\n",
    "        gnd_id2name[i['Code']]=gnd_id2name[i['Code']]+' '+j\n",
    "print(i['Code'],gnd_id2name[i['Code']])\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "print('len(train_datas)=',len(train_datas))\n",
    "# print('len(dev_datas)=',len(dev_datas))\n",
    "datas=train_datas\n",
    "print('len(datas)=',len(datas))\n",
    "# datas数据样本随机打乱\n",
    "import random\n",
    "print(datas[0])\n",
    "random.shuffle(datas)\n",
    "print(datas[0])\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "# with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "#     y_t=pickle.load(f)\n",
    "\n",
    "model_name_or_path='models/mpnet_gnd_triplet' #\"gte-multilingual-base\"\n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)\n",
    "\n",
    "def process_data(datas,result):\n",
    "    batch_size=768\n",
    "    texts=[datas[i:i+batch_size] for i in range(0, len(datas), batch_size)]\n",
    "    if len(texts)*batch_size<len(datas):\n",
    "        texts.append([datas[len(texts)*batch_size:]])\n",
    "    for block in texts:\n",
    "        text=[t['title']+'.'+t['abstract'] for t in block]\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "        with torch.no_grad():\n",
    "            similarities=torch.tensor(similarities).to('cuda')\n",
    "            # print(similarities.shape) 上次保存的前500个\n",
    "            top_k = torch.topk(similarities, k=20, dim=0).indices.T.cpu().tolist() # 取出相似度最高的20个\n",
    "            #similarities中的元素前500个取反\n",
    "            similarities=-similarities[:500]\n",
    "            top_k_last = torch.topk(similarities, k=20, dim=0).indices.T.cpu().tolist() # 取出相似度最低的20个\n",
    "        for i,t in enumerate(block):\n",
    "            pos=[]\n",
    "            for gnd in t['dcterms:subject']:\n",
    "                if type(gnd)==dict:\n",
    "                    gnd_id=gnd['@id']\n",
    "                else:\n",
    "                    continue\n",
    "                if gnd_id in gnd_id2name:\n",
    "                    pos.append(gnd_id2name[gnd_id])\n",
    "            for j in top_k[i]:\n",
    "                pos.append(gnd_id2name[y_t[j]['Code']])\n",
    "            item1={}\n",
    "            item1['pos']='.'.join(pos)\n",
    "            item1['neg']='. '.join([gnd_id2name[y_t[j]['Code']] for j in top_k_last[i]])\n",
    "            item1['text']=t['title']+'.'+t['abstract']\n",
    "            result.append(item1)\n",
    "\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "x_train=[]\n",
    "process_data(datas,x_train) \n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./tmp/train_core_mpnet.pkl','wb') as f:\n",
    "    pickle.dump(x_train,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "root='/media/jh/新加卷1'\n",
    "with open(f'{root}/2024_11_10/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# model_id='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "model_name_or_path='models/gte_gnd'#'/media/jh/新加卷/2025_1_16/sentence-transformers/all-mpnet-base-v2/' #models/gte_gnd 可以 \n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)#, \n",
    "# model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "def process_data(en_de,root,des_root,mode='m'):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        if not os.path.exists(f'{root}/{dirs}/{en_de}'):\n",
    "            continue\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            # if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "            #     continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            # 把file的后缀名去掉\n",
    "            file=file.split('.')[0]+'.json'\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    batch_size=768\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    index1=0\n",
    "    \n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "            # print(torch.tensor(similarities).shape) #(n,768)\n",
    "        if mode=='m2':\n",
    "            m2_similarities.extend(torch.tensor(similarities).T.tolist())\n",
    "            print(torch.tensor(m2_similarities).shape)\n",
    "            continue\n",
    "        elif mode=='m1':\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=500, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                sims=[]\n",
    "                for i in t:\n",
    "                    sims.append(similarities[index1][i])\n",
    "                index1+=1\n",
    "                top_k_2=torch.topk(torch.tensor(sims), k=50, dim=0).indices.T.cpu().tolist()\n",
    "                code=[]\n",
    "                for i in top_k_2:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=50, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                code=[]\n",
    "                for i in t:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            # if os.path.exists(file_names[index]):\n",
    "            #     index+=1\n",
    "            #     continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "        print('batch',index)\n",
    "        result=[] #重置result\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "root1='/media/jh/新加卷1'\n",
    "root='/media/jh/新加卷1/2024_11_10/llms4subjects-main/shared-task-datasets/TIBKAT/all-subjects/data/test/'\n",
    "des_root='/media/jh/新加卷1/2024_12_04/2025/gte_gnd_test'\n",
    "# model_id='./models/mpnet_gnd_triplet'#'/media/jh/新加卷/2025_1_16/final_model'\n",
    "# # with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "# #     y_t=pickle.load(f)\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "\n",
    "# m2_similarities=[]\n",
    "process_data('en',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_en.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "# m2_similarities=[]\n",
    "process_data('de',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_de.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0220, -0.0606, -0.0200,  ...,  0.0100, -0.0632,  0.0414],\n",
      "        [-0.0707, -0.0256, -0.0237,  ...,  0.0167, -0.0672, -0.0280],\n",
      "        [-0.0512, -0.0390,  0.0042,  ..., -0.0457, -0.1104,  0.0361]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "204739 204739\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 9996\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 10752\n",
      "batch 11520\n",
      "batch 12288\n",
      "batch 13056\n",
      "batch 13824\n",
      "batch 14592\n",
      "batch 15360\n",
      "batch 16128\n",
      "batch 16896\n",
      "batch 17664\n",
      "batch 17990\n",
      "运行时间: 1043.0688846111298 秒\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "root='/media/jh/新加卷'\n",
    "with open(f'{root}/2024_11_10/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# model_id='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "model_name_or_path='/media/jh/新加卷/2025_1_16/all-MiniLM-L6-v2/' #models/gte_gnd 可以 \n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)#, \n",
    "# model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "def process_data(en_de,root,des_root,mode='m'):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        if not os.path.exists(f'{root}/{dirs}/{en_de}'):\n",
    "            continue\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            # if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "            #     continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            # 把file的后缀名去掉\n",
    "            file=file.split('.')[0]+'.json'\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    batch_size=768\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    index1=0\n",
    "    \n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "            # print(torch.tensor(similarities).shape) #(n,768)\n",
    "        if mode=='m2':\n",
    "            m2_similarities.extend(torch.tensor(similarities).T.tolist())\n",
    "            print(torch.tensor(m2_similarities).shape)\n",
    "            continue\n",
    "        elif mode=='m1':\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=500, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                sims=[]\n",
    "                for i in t:\n",
    "                    sims.append(similarities[index1][i])\n",
    "                index1+=1\n",
    "                top_k_2=torch.topk(torch.tensor(sims), k=50, dim=0).indices.T.cpu().tolist()\n",
    "                code=[]\n",
    "                for i in top_k_2:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=50, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                code=[]\n",
    "                for i in t:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            # if os.path.exists(file_names[index]):\n",
    "            #     index+=1\n",
    "            #     continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "        print('batch',index)\n",
    "        result=[] #重置result\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "root1='/media/jh/新加卷'\n",
    "root='/media/jh/新加卷/2024_11_10/llms4subjects-main/shared-task-datasets/TIBKAT/all-subjects/data/test/'\n",
    "des_root='/media/jh/新加卷/2024_12_04/run-all-MiniLM-L6-v2'\n",
    "# model_id='./models/mpnet_gnd_triplet'#'/media/jh/新加卷/2025_1_16/final_model'\n",
    "# # with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "# #     y_t=pickle.load(f)\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "\n",
    "# m2_similarities=[]\n",
    "process_data('en',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_en.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "# m2_similarities=[]\n",
    "process_data('de',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_de.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0049, -0.0529,  0.0514,  ...,  0.0145,  0.0130,  0.0009],\n",
      "        [-0.0096, -0.0480,  0.0023,  ...,  0.0433, -0.0103,  0.0004],\n",
      "        [-0.0368, -0.0287,  0.0128,  ...,  0.0311, -0.0169,  0.0010]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "204739 204739\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 9996\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 10752\n",
      "batch 11520\n",
      "batch 12288\n",
      "batch 13056\n",
      "batch 13824\n",
      "batch 14592\n",
      "batch 15360\n",
      "batch 16128\n",
      "batch 16896\n",
      "batch 17664\n",
      "batch 17990\n",
      "运行时间: 1415.5852499008179 秒\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "root='/media/jh/新加卷'\n",
    "with open(f'{root}/2024_11_10/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# model_id='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "model_name_or_path='/media/jh/新加卷/2025_1_16/sentence-transformers/all-distilroberta-v1/' #models/gte_gnd 可以 \n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)#, \n",
    "# model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "def process_data(en_de,root,des_root,mode='m'):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        if not os.path.exists(f'{root}/{dirs}/{en_de}'):\n",
    "            continue\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            # if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "            #     continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            # 把file的后缀名去掉\n",
    "            file=file.split('.')[0]+'.json'\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    batch_size=768\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    index1=0\n",
    "    \n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "            # print(torch.tensor(similarities).shape) #(n,768)\n",
    "        if mode=='m2':\n",
    "            m2_similarities.extend(torch.tensor(similarities).T.tolist())\n",
    "            print(torch.tensor(m2_similarities).shape)\n",
    "            continue\n",
    "        elif mode=='m1':\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=500, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                sims=[]\n",
    "                for i in t:\n",
    "                    sims.append(similarities[index1][i])\n",
    "                index1+=1\n",
    "                top_k_2=torch.topk(torch.tensor(sims), k=50, dim=0).indices.T.cpu().tolist()\n",
    "                code=[]\n",
    "                for i in top_k_2:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=50, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                code=[]\n",
    "                for i in t:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            # if os.path.exists(file_names[index]):\n",
    "            #     index+=1\n",
    "            #     continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "        print('batch',index)\n",
    "        result=[] #重置result\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "root1='/media/jh/新加卷'\n",
    "root='/media/jh/新加卷/2024_11_10/llms4subjects-main/shared-task-datasets/TIBKAT/all-subjects/data/test/'\n",
    "des_root='/media/jh/新加卷/2024_12_04/run-all-distilroberta-v1'\n",
    "# model_id='./models/mpnet_gnd_triplet'#'/media/jh/新加卷/2025_1_16/final_model'\n",
    "# # with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "# #     y_t=pickle.load(f)\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "\n",
    "# m2_similarities=[]\n",
    "process_data('en',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_en.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "# m2_similarities=[]\n",
    "process_data('de',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_de.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0133, -0.0674, -0.0301,  ...,  0.0318, -0.0150, -0.0321],\n",
      "        [-0.0133, -0.0674, -0.0301,  ...,  0.0318, -0.0150, -0.0321],\n",
      "        [-0.0133, -0.0674, -0.0301,  ...,  0.0319, -0.0150, -0.0321]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "204739 204739\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 9996\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 10752\n",
      "batch 11520\n",
      "batch 12288\n",
      "batch 13056\n",
      "batch 13824\n",
      "batch 14592\n",
      "batch 15360\n",
      "batch 16128\n",
      "batch 16896\n",
      "batch 17664\n",
      "batch 17990\n",
      "运行时间: 1495.6767885684967 秒\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "root='/media/jh/新加卷'\n",
    "with open(f'{root}/2024_11_10/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# model_id='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "model_name_or_path='/media/jh/新加卷/2025_1_16/sentence-transformers/all-mpnet-base-v2/' #models/gte_gnd 可以 \n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)#, \n",
    "# model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "def process_data(en_de,root,des_root,mode='m'):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        if not os.path.exists(f'{root}/{dirs}/{en_de}'):\n",
    "            continue\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            # if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "            #     continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            # 把file的后缀名去掉\n",
    "            file=file.split('.')[0]+'.json'\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    batch_size=768\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    index1=0\n",
    "    \n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "            # print(torch.tensor(similarities).shape) #(n,768)\n",
    "        if mode=='m2':\n",
    "            m2_similarities.extend(torch.tensor(similarities).T.tolist())\n",
    "            print(torch.tensor(m2_similarities).shape)\n",
    "            continue\n",
    "        elif mode=='m1':\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=500, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                sims=[]\n",
    "                for i in t:\n",
    "                    sims.append(similarities[index1][i])\n",
    "                index1+=1\n",
    "                top_k_2=torch.topk(torch.tensor(sims), k=50, dim=0).indices.T.cpu().tolist()\n",
    "                code=[]\n",
    "                for i in top_k_2:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=50, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                code=[]\n",
    "                for i in t:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            # if os.path.exists(file_names[index]):\n",
    "            #     index+=1\n",
    "            #     continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "        print('batch',index)\n",
    "        result=[] #重置result\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "root1='/media/jh/新加卷'\n",
    "root='/media/jh/新加卷/2024_11_10/llms4subjects-main/shared-task-datasets/TIBKAT/all-subjects/data/test/'\n",
    "des_root='/media/jh/新加卷/2024_12_04/run-all-mpnet-base-v2'\n",
    "# model_id='./models/mpnet_gnd_triplet'#'/media/jh/新加卷/2025_1_16/final_model'\n",
    "# # with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "# #     y_t=pickle.load(f)\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "\n",
    "# m2_similarities=[]\n",
    "process_data('en',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_en.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "# m2_similarities=[]\n",
    "process_data('de',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_de.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2419,  0.8901, -0.3267,  ...,  0.0905, -0.6177, -0.5166],\n",
      "        [-0.4568,  0.6929, -0.4431,  ...,  0.1571, -0.6353, -0.3936],\n",
      "        [-0.4717,  0.7520, -0.5210,  ...,  0.2507, -0.7920, -0.3259]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "204739 204739\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 9996\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 10752\n",
      "batch 11520\n",
      "batch 12288\n",
      "batch 13056\n",
      "batch 13824\n",
      "batch 14592\n",
      "batch 15360\n",
      "batch 16128\n",
      "batch 16896\n",
      "batch 17664\n",
      "batch 17990\n",
      "运行时间: 1077.915846824646 秒\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "root='/media/jh/新加卷'\n",
    "with open(f'{root}/2024_11_10/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# model_id='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "model_name_or_path='/media/jh/新加卷/2025_1_16/sentence-transformers/paraphrase-MiniLM-L6-v2/' #models/gte_gnd 可以 \n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)#, \n",
    "# model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "def process_data(en_de,root,des_root,mode='m'):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        if not os.path.exists(f'{root}/{dirs}/{en_de}'):\n",
    "            continue\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            # if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "            #     continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            # 把file的后缀名去掉\n",
    "            file=file.split('.')[0]+'.json'\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    batch_size=768\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    index1=0\n",
    "    \n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "            # print(torch.tensor(similarities).shape) #(n,768)\n",
    "        if mode=='m2':\n",
    "            m2_similarities.extend(torch.tensor(similarities).T.tolist())\n",
    "            print(torch.tensor(m2_similarities).shape)\n",
    "            continue\n",
    "        elif mode=='m1':\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=500, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                sims=[]\n",
    "                for i in t:\n",
    "                    sims.append(similarities[index1][i])\n",
    "                index1+=1\n",
    "                top_k_2=torch.topk(torch.tensor(sims), k=50, dim=0).indices.T.cpu().tolist()\n",
    "                code=[]\n",
    "                for i in top_k_2:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=50, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                code=[]\n",
    "                for i in t:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            # if os.path.exists(file_names[index]):\n",
    "            #     index+=1\n",
    "            #     continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "        print('batch',index)\n",
    "        result=[] #重置result\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "root1='/media/jh/新加卷'\n",
    "root='/media/jh/新加卷/2024_11_10/llms4subjects-main/shared-task-datasets/TIBKAT/all-subjects/data/test/'\n",
    "des_root='/media/jh/新加卷/2024_12_04/run-paraphrase-MiniLM-L6-v2'\n",
    "# model_id='./models/mpnet_gnd_triplet'#'/media/jh/新加卷/2025_1_16/final_model'\n",
    "# # with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "# #     y_t=pickle.load(f)\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "\n",
    "# m2_similarities=[]\n",
    "process_data('en',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_en.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "# m2_similarities=[]\n",
    "process_data('de',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_de.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0318,  0.1566,  0.0232,  ...,  0.2311,  0.1600,  0.0582],\n",
      "        [-0.1687,  0.1361,  0.0512,  ...,  0.0435,  0.0306,  0.0728],\n",
      "        [-0.0496,  0.3281, -0.0147,  ..., -0.0675, -0.0553,  0.1239]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "204739 204739\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 9996\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7680\n",
      "batch 8448\n",
      "batch 9216\n",
      "batch 9984\n",
      "batch 10752\n",
      "batch 11520\n",
      "batch 12288\n",
      "batch 13056\n",
      "batch 13824\n",
      "batch 14592\n",
      "batch 15360\n",
      "batch 16128\n",
      "batch 16896\n",
      "batch 17664\n",
      "batch 17990\n",
      "运行时间: 1034.0145630836487 秒\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "root='/media/jh/新加卷'\n",
    "with open(f'{root}/2024_11_10/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# model_id='/home/jh/semeval/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/'\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "model_name_or_path='/media/jh/新加卷/2025_1_16/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/' #models/gte_gnd 可以 \n",
    "model = SentenceTransformer(model_name_or_path, model_kwargs={\"torch_dtype\": torch.float16}, trust_remote_code=True)#, \n",
    "# model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "def process_data(en_de,root,des_root,mode='m'):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        if not os.path.exists(f'{root}/{dirs}/{en_de}'):\n",
    "            continue\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            # if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "            #     continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            # 把file的后缀名去掉\n",
    "            file=file.split('.')[0]+'.json'\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    batch_size=768\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    index1=0\n",
    "    \n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                similaritie = model.similarity(y_tt.to(torch.float16),embeddings.to(torch.float16)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "            # print(torch.tensor(similarities).shape) #(n,768)\n",
    "        if mode=='m2':\n",
    "            m2_similarities.extend(torch.tensor(similarities).T.tolist())\n",
    "            print(torch.tensor(m2_similarities).shape)\n",
    "            continue\n",
    "        elif mode=='m1':\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=500, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                sims=[]\n",
    "                for i in t:\n",
    "                    sims.append(similarities[index1][i])\n",
    "                index1+=1\n",
    "                top_k_2=torch.topk(torch.tensor(sims), k=50, dim=0).indices.T.cpu().tolist()\n",
    "                code=[]\n",
    "                for i in top_k_2:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                similarities=torch.tensor(similarities).to('cuda')\n",
    "                # print(similarities.shape) 上次保存的前500个\n",
    "                top_k = torch.topk(similarities, k=50, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "            for t in top_k:\n",
    "                code=[]\n",
    "                for i in t:\n",
    "                    code.append(y_t[i]['Code'])\n",
    "                item1={}\n",
    "                item1['dcterms:subject']=code\n",
    "                result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            # if os.path.exists(file_names[index]):\n",
    "            #     index+=1\n",
    "            #     continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "        print('batch',index)\n",
    "        result=[] #重置result\n",
    "# 打印运行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "root1='/media/jh/新加卷'\n",
    "root='/media/jh/新加卷/2024_11_10/llms4subjects-main/shared-task-datasets/TIBKAT/all-subjects/data/test/'\n",
    "des_root='/media/jh/新加卷/2024_12_04/run-paraphrase-multilingual-MiniLM-L12-v2'\n",
    "# model_id='./models/mpnet_gnd_triplet'#'/media/jh/新加卷/2025_1_16/final_model'\n",
    "# # with open('./tmp/core_labels_gte.pkl','rb') as f:\n",
    "# #     y_t=pickle.load(f)\n",
    "# model = SentenceTransformer(model_id, model_kwargs={\"torch_dtype\": torch.float16, \"device\": \"cuda\"}) \n",
    "\n",
    "# m2_similarities=[]\n",
    "process_data('en',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_en.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "# m2_similarities=[]\n",
    "process_data('de',root,des_root,mode='m') #15904\n",
    "# with open('./tmp/m2_de.pkl', 'wb') as f:\n",
    "#     pickle.dump(m2_similarities, f)\n",
    "end_time = time.time()\n",
    "print('运行时间:', end_time - start_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6045, 79427])\n"
     ]
    }
   ],
   "source": [
    "m2_similarities=[]\n",
    "with open('./tmp/m2_en.pkl', 'rb') as f:\n",
    "    m2_similarities = pickle.load(f)   \n",
    "print(torch.tensor(m2_similarities).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m2_similarities=[]\n",
    "# with open('./tmp/m2_de.pkl', 'rb') as f:\n",
    "#     m2_similarities = pickle.load(f)\n",
    "len(m2_similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 09:20:24.653957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737076824.666171  100228 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737076824.670215  100228 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-17 09:20:24.683252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  28,  -16,  -73,  -84,  -50, -111,   59,  -43,   50,  -90,   88,   60,\n",
      "          -75,  126,  124,   25,   28,  -91,  -83,  -59,   98,  -49,  108, -120,\n",
      "          109, -103,  -57, -120, -102,  102,  -96, -118,   10,   65,  -99,  -50,\n",
      "           83,  -54,  118,  -10,   44,  111,  -47,  -39,   65,  -17,   72,   86,\n",
      "          -48,  -59,   -5,   84,  -38,   55,  -54,    0,   70,   56,  120,  126,\n",
      "          -85,   71,  -91,  101,  107,  -24,   -1,  -76,   14,  -63,   40,  107,\n",
      "          125,  -52, -117,    7, -115, -106,   83,   93,   45,  -52,   20,  -94,\n",
      "           82, -124,   -6,  -16,  -32,   96,   28,  102,  -44,   65,   11,   -9],\n",
      "        [  28,   -4,  -11,  -90,  -56, -111,   59,  -11,   62,  -74,  -56,   28,\n",
      "          -99,  -82,  127,   37,  124,  -83,  -83,  -59,  -32,   79,    8,    8,\n",
      "          101,  -71,   71, -120,   30,   98,  -90, -102,   11,   64, -107,  -86,\n",
      "          -46,  -62,  -12,  -74,   38,  103,  -16,   65,   73,  -25,   72,   68,\n",
      "          116,  -43,   -7,   20,  -40,   55,  -50,    8,    6,   56,   24,  110,\n",
      "          -53,  -57,  -83,  103,   99,  -24,   -1,  126, -114,  -61,   40,  -30,\n",
      "          125,  108,  -97,    7,   37, -107,  115,   93,   61,   76,   20,  -94,\n",
      "           82,   44,   -8,  -14,  -32,   64,   28,   46, -106,   98, -119,   -9],\n",
      "        [-105,  -68,   -9,  -94,  -46,   81,   50,  -11,   50,  -74,  -56,   60,\n",
      "          -67,  -18,  125,   57,   57,  -83,  -19, -123,  -64,   75,   40, -117,\n",
      "          101,  -79, -121,    8,   46,   98,  -30, -101,    9,   64, -127,  -86,\n",
      "          -45,  -61,  -12,  -74,   36,  -25,  -14,  -63,   77,  -81,   77,   85,\n",
      "          -12,  -59,  -11, -108,  -40,   55,  -54,   40, -122,   56,  120,  110,\n",
      "         -117,   71,   41,  101,  105,  -55,   -1,   22, -106,  -61,   40,  -32,\n",
      "          125,  100,  -81,    7,   37, -107,  115,   89,  -91,  -52,   16,  -77,\n",
      "           88, -116,  126,  -16,   32,  -64,   60,   46,  -75,   83, -119,   -5]],\n",
      "       dtype=torch.int8)\n",
      "204739 204739\n"
     ]
    }
   ],
   "source": [
    "# 生成all标签嵌入向量 int8量化\n",
    "import os\n",
    "import json\n",
    "\n",
    "root='/media/jh/新加卷'\n",
    "# model_id='/media/jh/新加卷/2024_12_04/gnd_model_result'\n",
    "model_id='/media/jh/新加卷/2024_12_04/gnd_models'\n",
    "with open(f'{root}/2024_12_04/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-all.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "import torch\n",
    "\n",
    "\n",
    "model = SentenceTransformer(model_id)\n",
    "model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True,precision=\"binary\")\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "import pickle\n",
    "with open('./tmp/all_labels_int8.pkl','wb') as f:\n",
    "    pickle.dump(y_t,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6045\n",
      "batch 768\n",
      "batch 1536\n",
      "batch 2304\n",
      "batch 3072\n",
      "batch 3840\n",
      "batch 4608\n",
      "batch 5376\n",
      "batch 6144\n",
      "batch 6912\n",
      "batch 7621\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# model_id='/media/jh/新加卷/2024_12_04/gnd_model_result'\n",
    "model_id='/media/jh/新加卷/2024_12_04/gnd_models'\n",
    "en_de='de'\n",
    "root='/media/jh/新加卷/2024_12_04/llms4subjects-main/shared-task-datasets/TIBKAT/all-subjects/data/dev'\n",
    "des_root='test/test_core1'\n",
    "\n",
    "with open('./tmp/all_labels_int8.pkl','rb') as f:\n",
    "    y_t=pickle.load(f)\n",
    "\n",
    "model = SentenceTransformer(model_id) #sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
    "model=model.to('cuda')\n",
    "\n",
    "def process_data(en_de,root,des_root):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "                continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    batch_size=768\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True, precision=\"binary\")\n",
    "        y_list=[y['embedding'].tolist() for y in y_t]\n",
    "        # y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        similarities=[]\n",
    "        embeddings=embeddings.to('cuda')\n",
    "        for cc in range(0,len(y_list),batch_size):\n",
    "            y_tt=torch.tensor(y_list[cc:cc+batch_size]).to('cuda')\n",
    "\n",
    "            similaritie = model.similarity(y_tt.to(torch.float32),embeddings.to(torch.float32)).to('cpu')\n",
    "            # print(similaritie.shape)\n",
    "            similarities.extend(similaritie.tolist())\n",
    "        \n",
    "        similarities=torch.tensor(similarities).to('cuda')\n",
    "        # print(similarities.shape) 上次保存的前500个\n",
    "        top_k = torch.topk(similarities, k=50, dim=0).indices.T.cpu().tolist() # 取出相似度最高的50个\n",
    "        for t in top_k:\n",
    "            code=[]\n",
    "            for i in t:\n",
    "                code.append(y_t[i]['Code'])\n",
    "            item1={}\n",
    "            item1['dcterms:subject']=code\n",
    "            result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            if os.path.exists(file_names[index]):\n",
    "                index+=1\n",
    "                continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "        print('batch',index)\n",
    "        result=[] #重置result\n",
    "    # print('数据处理完成，开始写入文件')\n",
    "    # for i in range(len(result)):\n",
    "    #     # 如果不存在目录，则创建目录\n",
    "    #     if os.path.exists(os.path.dirname(file_names[i])):\n",
    "    #         continue\n",
    "    #     if not os.path.exists(des_path[i]):\n",
    "    #         os.makedirs(des_path[i])\n",
    "    #     with open(file_names[i],'w') as f:\n",
    "    #         json.dump(result[i],f,ensure_ascii=False,indent=4)\n",
    "\n",
    "process_data('en',root,des_root) #15904\n",
    "process_data('de',root,des_root) #24165 24min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# tmp=[torch.zeros(3,5),torch.zeros(3,5),torch.zeros(3,5),torch.zeros(2,5)]\n",
    "# tmp1=[]\n",
    "# for i in range(4):\n",
    "#     tmp1.extend(tmp[i].tolist())\n",
    "# tmp1=torch.tensor(tmp1)\n",
    "# print(tmp1.shape)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def cosine_similarity_custom(tensor1, tensor2):\n",
    "    # 计算两个张量的点积\n",
    "    dot_product = torch.sum(tensor1 * tensor2 ,dtype=tensor1.dtype)\n",
    "    \n",
    "    # 计算两个张量的范数（L2范数）\n",
    "    norm1 = torch.sqrt(torch.sum(tensor1 ** 2, dtype=tensor1.dtype))\n",
    "    norm2 = torch.sqrt(torch.sum(tensor2 ** 2, dtype=tensor2.dtype))\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    cosine_sim = dot_product / (norm1 * norm2)\n",
    "    \n",
    "    return cosine_sim\n",
    "# 创建两个 int8 类型的张量\n",
    "tensor1 = torch.tensor([[1, 2, 3]], dtype=torch.int8)\n",
    "tensor2 = torch.tensor([4, 5, 6], dtype=torch.int8)\n",
    "\n",
    "cos_sim = cosine_similarity_custom(tensor1, tensor2)\n",
    "\n",
    "print(type(cos_sim.item()))\n",
    "print(cos_sim.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 21:32:13.777301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736775133.950706   24950 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736775133.989734   24950 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-13 21:32:14.370376: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0146, -0.1317, -0.0023,  ...,  0.1478,  0.0060, -0.1274],\n",
      "        [ 0.0770, -0.0802, -0.0041,  ..., -0.0109, -0.0813, -0.1333],\n",
      "        [-0.0050, -0.0863, -0.0048,  ...,  0.0784, -0.0179, -0.1176]],\n",
      "       device='cuda:0')\n",
      "79427 79427\n"
     ]
    }
   ],
   "source": [
    "# 生成core标签嵌入向量\n",
    "import os\n",
    "import json\n",
    "\n",
    "root='/media/jh/新加卷'\n",
    "with open(f'{root}/2024_12_04/llms4subjects-main/shared-task-datasets/GND/dataset/GND-Subjects-tib-core.json') as f:\n",
    "    gnd_jsons=json.load(f)\n",
    "labels=[]\n",
    "for c in gnd_jsons:\n",
    "    item1={}\n",
    "    item1['sentence2']='Classification Name is '+c['Classification Name']+'. Name is '+c['Name']\n",
    "    item1['Code']=c['Code']\n",
    "    labels.append(item1)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_id='/media/jh/新加卷/2024_12_04/gnd_model_result'\n",
    "\n",
    "model = SentenceTransformer(model_id) #sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
    "model=model.to('cuda')\n",
    "# 把labels的一维张量转换为二维张量\n",
    "batch_size=32\n",
    "texts=[label['sentence2'] for label in labels]\n",
    "texts=[texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "if len(texts)*batch_size<len(labels):\n",
    "    texts.append([label['sentence2'] for label in labels[len(texts)*batch_size:]])\n",
    "y_test=[]\n",
    "for text in texts:\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text, convert_to_tensor=True)\n",
    "    item=embeddings.cpu()\n",
    "    y_test.append(item)\n",
    "print(embeddings)\n",
    "\n",
    "y_t=[]\n",
    "i=0\n",
    "for y in y_test:\n",
    "    for e in y:\n",
    "        item={}\n",
    "        item['Code']=labels[i]['Code']\n",
    "        item['embedding']=e\n",
    "        item['sentence2']=labels[i]['sentence2']\n",
    "        y_t.append(item)\n",
    "        i+=1\n",
    "print(len(y_t),len(labels)) # \n",
    "\n",
    "import pickle\n",
    "with open(f'{root}/2024_12_04/llms4subjects-main/shared-task-datasets/core_labels.pkl','wb') as f:\n",
    "    pickle.dump(y_t,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "root1='/media/jh/新加卷'\n",
    "with open(f'{root1}/2024_12_04/llms4subjects-main/shared-task-datasets/core_labels.pkl','rb') as f:\n",
    "    y_t=pickle.load(f)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_id='/media/jh/新加卷/2024_12_04/gnd_model_result'\n",
    "\n",
    "model = SentenceTransformer(model_id) \n",
    "model=model.to('cuda')\n",
    "\n",
    "\n",
    "def process_data(en_de,root,des_root):\n",
    "    def get_train_data(json_file):\n",
    "        with open(json_file,'r') as f:\n",
    "            data=json.loads(f.read())\n",
    "        for d in data['@graph']:\n",
    "            if 'title' in d:\n",
    "                tmp=d\n",
    "                break\n",
    "        item={}\n",
    "        if type(tmp['title'])==list:\n",
    "            item['title']=' '.join(tmp['title'])\n",
    "        else:\n",
    "            item['title']=tmp['title']\n",
    "        if type(tmp['abstract'])==list:\n",
    "            item['abstract']='  '.join(tmp['abstract'])\n",
    "        else:\n",
    "            item['abstract']=tmp['abstract']\n",
    "        return item\n",
    "    x_test=[]\n",
    "    file_names=[]\n",
    "    des_path=[]\n",
    "    for dirs in os.listdir(f'{root}/'):\n",
    "        for file in os.listdir(f'{root}/{dirs}/{en_de}'):\n",
    "            if os.path.exists(f'{des_root}/{dirs}/{en_de}/{file}'):\n",
    "                continue\n",
    "            item=get_train_data(f'{root}/{dirs}/{en_de}/{file}')\n",
    "            text=item['title']+'.'+item['abstract']\n",
    "            x_test.append(text)\n",
    "            file_names.append(f'{des_root}/{dirs}/{en_de}/{file}')\n",
    "            des_path.append(f'{des_root}/{dirs}/{en_de}')\n",
    "    batch_size=64\n",
    "    texts=[x_test[i:i+batch_size] for i in range(0, len(x_test), batch_size)]\n",
    "    if len(texts)*batch_size<len(file_names):\n",
    "        texts.append([file_names[len(texts)*batch_size:]])\n",
    "    result=[]\n",
    "    index=0\n",
    "    for text in texts:\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.encode(text, convert_to_tensor=True)\n",
    "        y_tt=torch.tensor([y['embedding'].tolist() for y in y_t]).to('cuda')\n",
    "        # print(embeddings.shape,y_tt.shape)\n",
    "        result=[] #重置result\n",
    "        for embedding in embeddings:\n",
    "            with torch.no_grad():\n",
    "                cosine_similarity = F.cosine_similarity(embedding, y_tt, dim=1)\n",
    "                top_k = torch.topk(cosine_similarity, k=50, dim=0) # 取出相似度最高的50个\n",
    "                top_k=top_k.indices.cpu().tolist()\n",
    "            code=[]\n",
    "            for i in top_k:\n",
    "                code.append(y_t[i]['Code'])\n",
    "            item1={}\n",
    "            item1['dcterms:subject']=code\n",
    "            result.append(item1)\n",
    "        for res in result:\n",
    "            # 如果不存在目录，则创建目录\n",
    "            if not os.path.exists(des_path[index]):\n",
    "                os.makedirs(des_path[index])\n",
    "            if os.path.exists(file_names[index]):\n",
    "                index+=1\n",
    "                continue\n",
    "            with open(file_names[index],'w') as f:\n",
    "                json.dump(res,f,ensure_ascii=False,indent=4)\n",
    "            index+=1\n",
    "    # print('数据处理完成，开始写入文件')\n",
    "    # for i in range(len(result)):\n",
    "    #     # 如果不存在目录，则创建目录\n",
    "    #     if os.path.exists(os.path.dirname(file_names[i])):\n",
    "    #         continue\n",
    "    #     if not os.path.exists(des_path[i]):\n",
    "    #         os.makedirs(des_path[i])\n",
    "    #     with open(file_names[i],'w') as f:\n",
    "    #         json.dump(result[i],f,ensure_ascii=False,indent=4)\n",
    "en_de='de'\n",
    "root='./test-tib-core'\n",
    "des_root=f'{root1}/2024_12_04/result_core'\n",
    "process_data('en',root,des_root)\n",
    "process_data('de',root,des_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
